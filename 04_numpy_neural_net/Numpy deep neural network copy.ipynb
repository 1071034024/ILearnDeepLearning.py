{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's code a Neural Network in plain NumPy\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Author: Piotr Skalski***\n",
    "\n",
    "Using high-level frameworks like Keras, TensorFlow or PyTorch allows us to build very complex models quickly. However, it is worth taking the time to look inside and understand underlying concepts. Not so long ago I published an article in which I tried to explain - in a simple way - how neural nets work. However, it was  highly theoretical post, dedicated primarily to math, which is the source of NN superpower. From the very beginning I was planning to follow up this topic in a more practical way. This time we will try to make use of our knowledge and build a fully operational neural network using only NumPy. Finally, we will also use it to solve simple classification problems and compare its performance with the model built with Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the file name (required)\n",
    "__file__ = 'Numpy deep neural network.ipynb'\n",
    "\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from IPython.display import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import ipytest.magics\n",
    "import pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Network architecture](./supporting_visualizations/nn_architecture.png)\n",
    "\n",
    "<b>Figure 1.</b> Example of dense neural network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First things first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start programming, let's stop for a moment and prepare a basic roadmap. Our goal is to create a program capable of creating a densely connected neural network with the specified architecture (number and size of layers and appropriate activation function). An example of such a network is presented in Figure 1. Above all, we must be able to train our network and make predictions using it.\n",
    "\n",
    "![Roadmap](./supporting_visualizations/blueprint.gif)\n",
    "\n",
    "<b>Figure 2.</b> Neural network blueprint\n",
    "\n",
    "Diagram above shows what operations will have to be performed during the training of our neural network. It also shows how many parameters we will have to update and read at different stages of a single iteration. Building the right data structure and skillfully managing its state is the most difficult part of our task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiation of neural network layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Parameters sizes](./supporting_visualizations/params_sizes.png)\n",
    "\n",
    "<b>Figure 3.</b> Dimensions of weight matrix W and bias vector b for layer l.\n",
    "\n",
    "Let's start with by initiating weight matrix W and bias vector b for each layer. In Figure 3 I have prepared a small cheatsheet, which will help us to asign the appropriate dimensions for these coefficients. Superscript [l] denotes the index of the current layer (counted from 1). I assumed that the information describing the NN architecture will be delivered to our program in the form of list similar to the one presented on Snippet 1. Each item in the list is a dictionary describing the basic parameters of a single network layer: input_dim - the size of the signal vector supplied as an input for the layer, output_dim - the size of the activation vector obtained at the output of the layer and activation - the activation function to be used inside the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_ARCHITECTURE = [\n",
    "    {\"input_dim\": 2, \"output_dim\": 50, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 50, \"output_dim\": 100, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 100, \"output_dim\": 200, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 200, \"output_dim\": 100, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 100, \"output_dim\": 50, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 50, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Activations](./supporting_visualizations/activations.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0;\n",
    "    return dZ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiation of parameter values for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layers(nn_architecture, seed = 99):\n",
    "    # random seed initiation\n",
    "    np.random.seed(seed)\n",
    "    # number of layers in our neural network\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    # parameters storage initiation\n",
    "    params_values = {}\n",
    "    \n",
    "    # iteration over network layers\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        # we number network layers from 1\n",
    "        layer_idx = idx + 1\n",
    "        \n",
    "        # extracting the number of units in layers\n",
    "        layer_input_size = layer[\"input_dim\"]\n",
    "        layer_output_size = layer[\"output_dim\"]\n",
    "        \n",
    "        # initiating the values of the W matrix\n",
    "        # and vector b for subsequent layers\n",
    "        params_values['W' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, layer_input_size) * 0.1\n",
    "        params_values['b' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, 1) * 0.1\n",
    "        \n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..                                                                         [100%]\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "# TEST PARAMETERS SHAPES\n",
    "\n",
    "params_values = init_layers(NN_ARCHITECTURE)\n",
    "\n",
    "def test_first_layer_W_shape():\n",
    "    assert params_values[\"W1\"].shape == (NN_ARCHITECTURE[0][\"output_dim\"], NN_ARCHITECTURE[0][\"input_dim\"])\n",
    "def test_first_layer_b_shape():\n",
    "    assert params_values[\"b1\"].shape == (NN_ARCHITECTURE[0][\"output_dim\"], 1)\n",
    "def test_first_layer_W_shape():\n",
    "    assert params_values[\"W2\"].shape == (NN_ARCHITECTURE[1][\"output_dim\"], NN_ARCHITECTURE[1][\"input_dim\"])\n",
    "def test_first_layer_b_shape():\n",
    "    assert params_values[\"b2\"].shape == (NN_ARCHITECTURE[1][\"output_dim\"], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single layer forward propagation step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Single unit](./supporting_visualizations/single_unit.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n",
    "    # calculation of the input value for the activation function\n",
    "    Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "    \n",
    "    # selection of activation function\n",
    "    if activation is \"relu\":\n",
    "        activation_func = relu\n",
    "    elif activation is \"sigmoid\":\n",
    "        activation_func = sigmoid\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "        \n",
    "    # return of calculated activation A and the intermediate Z matrix\n",
    "    return activation_func(Z_curr), Z_curr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Matrix sizes](./supporting_visualizations/matrix_sizes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Matrix sizes 2](./supporting_visualizations/matrix_sizes_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....                                                                       [100%]\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "# TEST OUTPUT FOR SINGLE LAYER FORWARD STEP\n",
    "\n",
    "np.random.seed(2)\n",
    "A_prev = np.random.randn(3,2)\n",
    "W_curr = np.random.randn(1,3)\n",
    "b_curr = np.random.randn(1,1)\n",
    "\n",
    "A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\")\n",
    "\n",
    "def test_relu_Z_shape():\n",
    "    assert Z_curr.shape == (1,2)\n",
    "def test_relu_A_shape():\n",
    "    assert A_curr.shape == (1,2)\n",
    "def test_relu_Z_value():\n",
    "    assert np.allclose(Z_curr, np.array([[ 3.43896131, -2.08938436]]))\n",
    "def test_relu_A_value():\n",
    "    assert np.allclose(A_curr, np.array([[3.43896131, 0.        ]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_forward_propagation(X, params_values, nn_architecture):\n",
    "    # creating a temporary memory to store the information needed for a backward step\n",
    "    memory = {}\n",
    "    # X vector is the activation for layer 0 \n",
    "    A_curr = X\n",
    "    \n",
    "    # iteration over network layers\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        # we number network layers from 1\n",
    "        layer_idx = idx + 1\n",
    "        # transfer the activation from the previous iteration\n",
    "        A_prev = A_curr\n",
    "        \n",
    "        # extraction of the activation function for the current layer\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        # extraction of W for the current layer\n",
    "        W_curr = params_values[\"W\" + str(layer_idx)]\n",
    "        # extraction of b for the current layer\n",
    "        b_curr = params_values[\"b\" + str(layer_idx)]\n",
    "        # calculation of activation for the current layer\n",
    "        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n",
    "        \n",
    "        # saving calculated values in the memory\n",
    "        memory[\"A\" + str(idx)] = A_prev\n",
    "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "       \n",
    "    # return of prediction vector and a dictionary containing intermediate values\n",
    "    return A_curr, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[-0.31178367,  0.72900392,  0.21782079, -0.8990918 ],\n",
      "       [-2.48678065,  0.91325152,  1.12706373, -1.51409323],\n",
      "       [ 1.63929108, -0.4298936 ,  2.63128056,  0.60182225],\n",
      "       [-0.33588161,  1.23773784,  0.11112817,  0.12915125],\n",
      "       [ 0.07612761, -0.15512816,  0.63422534,  0.810655  ]])\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6)\n",
    "X = np.random.randn(5, 4)\n",
    "W1 = np.random.randn(4, 5)\n",
    "b1 = np.random.randn(4, 1)\n",
    "W2 = np.random.randn(3, 4)\n",
    "b2 = np.random.randn(3, 1)\n",
    "W3 = np.random.randn(1, 3)\n",
    "b3 = np.random.randn(1, 1)\n",
    "\n",
    "params_values = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"W3\": W3,\n",
    "                \"b3\": b3}\n",
    "pprint(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_architecture = [\n",
    "    {\"input_dim\": 4, \"output_dim\": 5, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 4, \"output_dim\": 3, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 3, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, memory = full_forward_propagation(X, params_values, nn_architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A0': array([[-0.31178367,  0.72900392,  0.21782079, -0.8990918 ],\n",
      "       [-2.48678065,  0.91325152,  1.12706373, -1.51409323],\n",
      "       [ 1.63929108, -0.4298936 ,  2.63128056,  0.60182225],\n",
      "       [-0.33588161,  1.23773784,  0.11112817,  0.12915125],\n",
      "       [ 0.07612761, -0.15512816,  0.63422534,  0.810655  ]]),\n",
      " 'A1': array([[0.        , 3.18040136, 0.4074501 , 0.        ],\n",
      "       [0.        , 0.        , 3.18141623, 0.        ],\n",
      "       [4.18500916, 0.        , 0.        , 2.72141638],\n",
      "       [5.05850802, 0.        , 0.        , 3.82321852]]),\n",
      " 'A2': array([[ 2.2644603 ,  1.09971298,  0.        ,  1.54036335],\n",
      "       [ 6.33722569,  0.        ,  0.        ,  4.48582383],\n",
      "       [10.37508342,  0.        ,  1.63635185,  8.17870169]]),\n",
      " 'Z1': array([[-5.23825714,  3.18040136,  0.4074501 , -1.88612721],\n",
      "       [-2.77358234, -0.56177316,  3.18141623, -0.99209432],\n",
      "       [ 4.18500916, -1.78006909, -0.14502619,  2.72141638],\n",
      "       [ 5.05850802, -1.25674082, -3.54566654,  3.82321852]]),\n",
      " 'Z2': array([[ 2.2644603 ,  1.09971298, -2.90298027,  1.54036335],\n",
      "       [ 6.33722569, -2.38116246, -4.11228806,  4.48582383],\n",
      "       [10.37508342, -0.66591468,  1.63635185,  8.17870169]]),\n",
      " 'Z3': array([[-3.19864676,  0.87117055, -1.40297864, -3.00319435]])}\n"
     ]
    }
   ],
   "source": [
    "pprint(memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the value of the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cost_value(Y_hat, Y):\n",
    "    # number of examples\n",
    "    m = Y_hat.shape[1]\n",
    "    # calculation of the cost according to the formula\n",
    "    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.4149316)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.asarray([[1, 1, 1]])\n",
    "Y_hat = np.array([[.8, .9, 0.4]])\n",
    "get_cost_value(Y_hat, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single layer backward propagation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
    "    # number of examples\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    # selection of activation function\n",
    "    if activation is \"relu\":\n",
    "        backward_activation_func = relu_backward\n",
    "    elif activation is \"sigmoid\":\n",
    "        backward_activation_func = sigmoid_backward\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "        \n",
    "    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
    "    \n",
    "    # derivative of the matrix W\n",
    "    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
    "    # derivative of the vector b\n",
    "    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
    "    # derivative of the matrix A_prev\n",
    "    dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "\n",
    "    return dA_prev, dW_curr, db_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.44090989,  0.        ],\n",
       "        [ 0.37883606,  0.        ],\n",
       "        [-0.2298228 ,  0.        ]]),\n",
       " array([[ 0.44513824,  0.37371418, -0.10478989]]),\n",
       " array([[-0.20837892]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "dA_curr = np.random.randn(1, 2)\n",
    "A_prev = np.random.randn(3, 2)\n",
    "W_curr = np.random.randn(1, 3)\n",
    "b_curr = np.random.randn(1, 1)\n",
    "Z_curr = np.random.randn(1, 2)\n",
    "\n",
    "single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boldsymbol{dW}^{[l]} = \\frac{\\partial L }{\\partial \\boldsymbol{W}^{[l]}} = \\frac{1}{m} \\boldsymbol{dZ}^{[l]} \\boldsymbol{A}^{[l-1] T}$$\n",
    "\n",
    "$$\\boldsymbol{db}^{[l]} = \\frac{\\partial L }{\\partial \\boldsymbol{b}^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} \\boldsymbol{dZ}^{[l](i)}$$\n",
    "\n",
    "$$\\boldsymbol{dA}^{[l-1]} = \\frac{\\partial L }{\\partial \\boldsymbol{A}^{[l-1]}} = \\boldsymbol{W}^{[l] T} \\boldsymbol{dZ}^{[l]}$$\n",
    "\n",
    "$$\\boldsymbol{dZ}^{[l]} = \\boldsymbol{dA}^{[l]} * g'(\\boldsymbol{Z}^{[l]})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):\n",
    "    grads_values = {}\n",
    "    \n",
    "    # number of examples\n",
    "    m = Y.shape[1]\n",
    "    # a hack ensuring the same shape of the prediction vector and labels vector\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "    \n",
    "    dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));\n",
    "    \n",
    "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
    "        # we number network layers from 1\n",
    "        layer_idx_curr = layer_idx_prev + 1\n",
    "        # extraction of the activation function for the current layer\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        \n",
    "        dA_curr = dA_prev\n",
    "        \n",
    "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
    "        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
    "        \n",
    "        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n",
    "        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n",
    "        \n",
    "        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n",
    "            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n",
    "        \n",
    "        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
    "        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
    "    \n",
    "    return grads_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dW1': array([[0.41010002, 0.07807203, 0.13798444, 0.10502167],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.05283652, 0.01005865, 0.01777766, 0.0135308 ]]),\n",
       " 'dW2': array([[-0.39202432, -0.13325855, -0.04601089]]),\n",
       " 'db1': array([[-0.22007063],\n",
       "        [ 0.        ],\n",
       "        [-0.02835349]]),\n",
       " 'db2': array([[0.15187861]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "Y_hat = np.random.randn(1, 2)\n",
    "Y = np.array([[1, 0]])\n",
    "\n",
    "A0 = np.random.randn(4, 2)\n",
    "W1 = np.random.randn(3, 4)\n",
    "b1 = np.random.randn(3, 1)\n",
    "Z1 = np.random.randn(3, 2)\n",
    "\n",
    "A1 = np.random.randn(3, 2)\n",
    "W2 = np.random.randn(1, 3)\n",
    "b2 = np.random.randn(1, 1)\n",
    "Z2 = np.random.randn(1, 2)\n",
    "\n",
    "params_values = {}\n",
    "params_values[\"W1\"] = W1\n",
    "params_values[\"b1\"] = b1\n",
    "params_values[\"W2\"] = W2\n",
    "params_values[\"b2\"] = b2\n",
    "\n",
    "memory = {}\n",
    "memory[\"A0\"] = A0\n",
    "memory[\"Z1\"] = Z1\n",
    "memory[\"A1\"] = A1\n",
    "memory[\"Z2\"] = Z2\n",
    "\n",
    "nn_architecture = [\n",
    "    {\"input_dim\": 4, \"output_dim\": 3, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 3, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]\n",
    "\n",
    "full_backward_propagation(Y_hat, Y, memory,  params_values, nn_architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
    "\n",
    "    # iteration over network layers\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        # we number network layers from 1\n",
    "        layer_idx = idx + 1\n",
    "        \n",
    "        params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
    "        params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "    return params_values;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[-0.59562069, -0.09991781, -2.14584584,  1.82662008],\n",
       "        [-1.76569676, -0.80627147,  0.51115557, -1.18258802],\n",
       "        [-1.0535704 , -0.86128581,  0.68284052,  2.20374577]]),\n",
       " 'W2': array([[-0.55569196,  0.0354055 ,  1.32964895]]),\n",
       " 'b1': array([[-0.04659241],\n",
       "        [-1.28888275],\n",
       "        [ 0.53405496]]),\n",
       " 'b2': array([[-0.84610769]])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "W1 = np.random.randn(3, 4)\n",
    "b1 = np.random.randn(3, 1)\n",
    "W2 = np.random.randn(1, 3)\n",
    "b2 = np.random.randn(1, 1)\n",
    "parameters = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2}\n",
    "np.random.seed(3)\n",
    "dW1 = np.random.randn(3, 4)\n",
    "db1 = np.random.randn(3, 1)\n",
    "dW2 = np.random.randn(1, 3)\n",
    "db2 = np.random.randn(1, 1)\n",
    "grads = {\"dW1\": dW1,\n",
    "         \"db1\": db1,\n",
    "         \"dW2\": dW2,\n",
    "         \"db2\": db2}\n",
    "\n",
    "nn_architecture = [\n",
    "    {\"input_dim\": 4, \"output_dim\": 3, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 3, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]\n",
    "\n",
    "update(parameters, grads, nn_architecture, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, nn_architecture, epochs, learning_rate):\n",
    "    params_values = init_layers(nn_architecture, 2)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "#     for i in tqdm(range(epochs)):\n",
    "#         print(params_values[\"W1\"][0])\n",
    "        Y_hat, cashe = full_forward_propagation(X, params_values, nn_architecture)\n",
    "        cost = get_cost_value(Y_hat, Y)\n",
    "        if(i % 200 == 0):\n",
    "            print(\"Iteration: {} - cost: {}\".format(cost, i))\n",
    "        grads_values = full_backward_propagation(Y_hat, Y, cashe, params_values, nn_architecture)\n",
    "#         print(grads_values[\"dW2\"][0])\n",
    "        params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
    "        \n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples in the data set\n",
    "N_SAMPLES = 1000\n",
    "# ratio between training and test sets\n",
    "TEST_SIZE = 0.1\n",
    "# number of iterations of the model\n",
    "N_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_circles(n_samples=N_SAMPLES, factor=.3, noise=.10)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0.6938129619229754 - cost: 0\n",
      "Iteration: 0.6895829493236217 - cost: 200\n",
      "Iteration: 0.6858955922538015 - cost: 400\n",
      "Iteration: 0.6801361868834316 - cost: 600\n",
      "Iteration: 0.6702499234379141 - cost: 800\n",
      "Iteration: 0.6493653695640564 - cost: 1000\n",
      "Iteration: 0.5967080898232197 - cost: 1200\n",
      "Iteration: 0.4389623136346585 - cost: 1400\n",
      "Iteration: 0.16235153585245227 - cost: 1600\n",
      "Iteration: 0.052629948923844705 - cost: 1800\n",
      "Iteration: 0.025107234623227174 - cost: 2000\n",
      "Iteration: 0.015233370185100945 - cost: 2200\n",
      "Iteration: 0.010561400874414954 - cost: 2400\n",
      "Iteration: 0.007927754253277684 - cost: 2600\n",
      "Iteration: 0.006273385746634368 - cost: 2800\n",
      "Iteration: 0.0051496810519635625 - cost: 3000\n",
      "Iteration: 0.004342984058184206 - cost: 3200\n",
      "Iteration: 0.003740084696233018 - cost: 3400\n",
      "Iteration: 0.003273886623793503 - cost: 3600\n",
      "Iteration: 0.002903615822674675 - cost: 3800\n",
      "Iteration: 0.002603240310480528 - cost: 4000\n",
      "Iteration: 0.0023553226784523156 - cost: 4200\n",
      "Iteration: 0.002148204576107743 - cost: 4400\n",
      "Iteration: 0.0019724749404655224 - cost: 4600\n",
      "Iteration: 0.0018213474782416707 - cost: 4800\n"
     ]
    }
   ],
   "source": [
    "params_values = train(np.transpose(X_train), np.transpose(y_train.reshape((y_train.shape[0], 1))), NN_ARCHITECTURE, 5000, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat, _ = full_forward_propagation(np.transpose(X_test), params_values, NN_ARCHITECTURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
       "        0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,\n",
       "        0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "        1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0.,\n",
       "        1., 1., 0., 1.]])"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat[y_hat > 0.5] = 1\n",
    "y_hat[y_hat <= 0.5] = 0\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.reshape((y_hat.shape[1], 1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_hat.reshape((y_hat.shape[1], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
